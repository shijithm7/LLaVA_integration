{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaVA 13B Model Inference\n",
    "This notebook demonstrates the inference process for the LLaVA 13B model, optimized with 4-bit quantization. It includes steps for setting up the environment, loading the model, and running inference with visual and textual inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Python Version\n",
    "Ensuring that the Python version is compatible with the LLaVA library is crucial for preventing compatibility issues. This check helps verify that our environment aligns with the requirements for running the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fIckzIvf93Sr",
    "outputId": "4463f9b1-5077-489a-b17c-542dda16fcbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\n"
     ]
    }
   ],
   "source": [
    "# Check the Python version to ensure compatibility with LLaVA requirements\n",
    "\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries\n",
    "This section involves importing all necessary libraries and modules that will be used throughout the notebook. These imports include handling images, managing conversation templates, and performing tokenization and inference, which are critical for interacting with the LLaVA model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0CIqSvD-WUmt",
    "outputId": "97195945-8c23-4f17-e66c-d8198a450f0d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shijith/anaconda3/envs/LLaVA_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import essential libraries and setup the environment for LLaVA model operations.\n",
    "# This includes utilities for handling images, managing conversation templates, and performing tokenization and inference\n",
    "\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llava.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\n",
    "from transformers import TextStreamer\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "from llava.model import LlavaLlamaForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Directory Contents\n",
    "Before proceeding, it's important to ensure that all necessary files and directories are present. Listing the contents of the current directory helps us verify that the environment is correctly set up for the tasks ahead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs\t LLaVA_13b_4bit_vanilla_inference_code.ipynb  README.md\n",
      "images\t llava-v1.5-13b-3GB\t\t\t      scripts\n",
      "LICENSE  playground\t\t\t\t      Untitled.ipynb\n",
      "llava\t pyproject.toml\n"
     ]
    }
   ],
   "source": [
    "!# Display the contents of the current working directory to verify the presence of necessary files and directories.\n",
    "\n",
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading and Configuration\n",
    "\n",
    "In this section, we initialize and load the LLaVA model with specific settings tailored for memory efficiency and performance. The model's path points to the version and configuration intended for deployment. Special attention is given to the quantization parameters to leverage 4-bit precision, which greatly reduces the memory footprint during inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191,
     "referenced_widgets": [
      "69441de3f3a54246bfafba2cdef6b3b0",
      "c25c06468da244bca7c7611cfa4f1114",
      "854a6e11901d4eb78bc1c080c7a07604",
      "e15bb373ca214346960cb6527d921fec",
      "f0f1d461dd674d4a986be7423681e412",
      "c34af28df6e64912b379578729dedc66",
      "e3ab31d9800043329fa9b1141b3dc24d",
      "8d5ef144ba4f49a69a397cc553f6e7eb",
      "d3686346db5342cb81f555e6f325dd2b",
      "d85050dd57f34c8a9bd69a0bdfb2087b",
      "56cb4279b9d54efc96a226f7ecd2a8a0"
     ]
    },
    "id": "S5Jw06slWO2z",
    "outputId": "a9025a53-4251-4873-9fab-3492e8693053"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|                          | 0/9 [00:00<?, ?it/s]/home/shijith/anaconda3/envs/LLaVA_env/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Loading checkpoint shards:  33%|██████            | 3/9 [01:14<02:29, 24.96s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_path = \"llava-v1.5-13b-3GB\"\n",
    "\n",
    "kwargs = {\"device_map\": \"cpu\"}\n",
    "kwargs['load_in_4bit'] = True\n",
    "kwargs['quantization_config'] = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.int8,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    load_in_8bit_fp32_cpu_offload=True,\n",
    "    bnb_4bit_quant_type='nf4'\n",
    ")\n",
    "\n",
    "model = LlavaLlamaForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_tower = model.get_vision_tower()\n",
    "if not vision_tower.is_loaded:\n",
    "    vision_tower.load_model()\n",
    "vision_tower.to(device='cuda')\n",
    "image_processor = vision_tower.image_processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Inference Function\n",
    "The `interact_image` function is defined here to facilitate the process of loading images, preprocessing them, and performing inference. The function takes an image path and a textual prompt as inputs, processes these inputs using the LLaVA model, and returns the model's generated response. This function exemplifies how to integrate and utilize the model's capabilities for practical applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2MZmkuBsUTHZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def interact_image(image_file, prompt):\n",
    "\n",
    "\n",
    "     \"\"\"\n",
    "    Function to load an image, preprocess it, and perform inference using the LLaVA model.\n",
    "    \n",
    "    Args:\n",
    "    image_path (str): The path to the image file.\n",
    "    prompt (str): The prompt to guide the model's response generation, including queries about the image.\n",
    "\n",
    "    Returns:\n",
    "    tuple: The original image and the model's textual output.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load and preprocess the image for inference\n",
    "    if image_file.startswith('http') or image_file.startswith('https'):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "    disable_torch_init()\n",
    "    conv_mode = \"llava_v0\"\n",
    "    conv = conv_templates[conv_mode].copy()\n",
    "    roles = conv.roles\n",
    "    image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'].half().cuda()\n",
    "\n",
    "    # Prepare the input prompt with role and token markers\n",
    "    inp = f\"{roles[0]}: {prompt}\"\n",
    "    inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + inp\n",
    "    # Initialize a conversation object and append initial messages\n",
    "    conv.append_message(conv.roles[0], inp)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    # Construct the full prompt and convert it to tensor for model input\n",
    "    raw_prompt = conv.get_prompt()\n",
    "    input_ids = tokenizer_image_token(raw_prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\n",
    "    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "    keywords = [stop_str]\n",
    "    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n",
    "    # Perform model inference in no_grad context to optimize memory usage\n",
    "    with torch.inference_mode():\n",
    "      output_ids = model.generate(input_ids, images=image_tensor, do_sample=True, temperature=0.2,\n",
    "                                  max_new_tokens=1024, use_cache=True, stopping_criteria=[stopping_criteria])\n",
    "    outputs = tokenizer.decode(output_ids[0, input_ids.shape[1]:]).strip()\n",
    "    conv.messages[-1][-1] = outputs\n",
    "    output = outputs.rsplit('</s>', 1)[0]\n",
    "    return image, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Inference Function\n",
    "The `interact_image` function is defined here to facilitate the process of loading images, preprocessing them, and performing inference. The function takes an image path and a textual prompt as inputs, processes these inputs using the LLaVA model, and returns the model's generated response. This function exemplifies how to integrate and utilize the model's capabilities for practical applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WbvpNRxTUTHc",
    "outputId": "b479cf46-6216-4084-cfa4-63af83158a40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image is a black and white drawing of a model, likely a 3D model, featuring a long object with a curve. The drawing includes various measurements and dimensions, such as 1.5\" and 1.25\". The measurements are provided in inches, and the drawing appears to be a blueprint or a technical drawing. The image also has a few notes, which might provide additional information or instructions for the model.\n"
     ]
    }
   ],
   "source": [
    "# Run inference to analyze an image and generate a description based on the provided prompt.\n",
    "# This demonstrates the model's ability to integrate visual and textual information.\n",
    "\n",
    "\n",
    "\n",
    "image, output = interact_image(f'Screenshot from 2024-08-24 13-52-43.png',\n",
    "'Describe the image and color details. as well what are this drawings? which dimensions are provided here? and what are the measurements available?'\n",
    ")\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "LLaVA_env",
   "language": "python",
   "name": "llava_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "56cb4279b9d54efc96a226f7ecd2a8a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "69441de3f3a54246bfafba2cdef6b3b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c25c06468da244bca7c7611cfa4f1114",
       "IPY_MODEL_854a6e11901d4eb78bc1c080c7a07604",
       "IPY_MODEL_e15bb373ca214346960cb6527d921fec"
      ],
      "layout": "IPY_MODEL_f0f1d461dd674d4a986be7423681e412"
     }
    },
    "854a6e11901d4eb78bc1c080c7a07604": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d5ef144ba4f49a69a397cc553f6e7eb",
      "max": 9,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d3686346db5342cb81f555e6f325dd2b",
      "value": 9
     }
    },
    "8d5ef144ba4f49a69a397cc553f6e7eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c25c06468da244bca7c7611cfa4f1114": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c34af28df6e64912b379578729dedc66",
      "placeholder": "​",
      "style": "IPY_MODEL_e3ab31d9800043329fa9b1141b3dc24d",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "c34af28df6e64912b379578729dedc66": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d3686346db5342cb81f555e6f325dd2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d85050dd57f34c8a9bd69a0bdfb2087b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e15bb373ca214346960cb6527d921fec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d85050dd57f34c8a9bd69a0bdfb2087b",
      "placeholder": "​",
      "style": "IPY_MODEL_56cb4279b9d54efc96a226f7ecd2a8a0",
      "value": " 9/9 [02:16&lt;00:00, 14.50s/it]"
     }
    },
    "e3ab31d9800043329fa9b1141b3dc24d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f0f1d461dd674d4a986be7423681e412": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
